{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtox72csOQUN"
      },
      "source": [
        "# DeepMatch 样例代码\n",
        "- https://github.com/shenweichen/DeepMatch\n",
        "- https://deepmatch.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWHz-heMkyw"
      },
      "source": [
        "# 下载movielens-1M数据 安装依赖包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTl6d6jO1oqf",
        "outputId": "6f4516af-0f03-4f35-8f0d-80e803021095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-17 04:30:51--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5917549 (5.6M) [application/zip]\n",
            "Saving to: ‘./ml-1m.zip’\n",
            "\n",
            "./ml-1m.zip         100%[===================>]   5.64M  3.47MB/s    in 1.6s    \n",
            "\n",
            "2022-06-17 04:30:54 (3.47 MB/s) - ‘./ml-1m.zip’ saved [5917549/5917549]\n",
            "\n",
            "--2022-06-17 04:30:54--  https://raw.githubusercontent.com/shenweichen/DeepMatch/master/examples/preprocess.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5642 (5.5K) [text/plain]\n",
            "Saving to: ‘preprocess.py’\n",
            "\n",
            "preprocess.py       100%[===================>]   5.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-06-17 04:30:54 (71.1 MB/s) - ‘preprocess.py’ saved [5642/5642]\n",
            "\n",
            "Archive:  ml-1m.zip\n",
            "  inflating: ml-1m/movies.dat        \n",
            "  inflating: ml-1m/ratings.dat       \n",
            "  inflating: ml-1m/README            \n",
            "  inflating: ml-1m/users.dat         \n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! wget http://files.grouplens.org/datasets/movielens/ml-1m.zip -O ./ml-1m.zip \n",
        "! wget https://raw.githubusercontent.com/shenweichen/DeepMatch/master/examples/preprocess.py -O preprocess.py\n",
        "! unzip -o ml-1m.zip \n",
        "! pip uninstall -y -q tensorflow\n",
        "! pip install -q tensorflow-gpu==2.5.0\n",
        "! pip install -q deepmatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9UxNHuPMuW2"
      },
      "source": [
        "# 导入需要的库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_ZR6gzp1E2N",
        "outputId": "903724ad-114b-4ea8-d0ce-151f9f6d4cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:\n",
            "DeepCTR version 0.9.1 detected. Your version is 0.8.2.\n",
            "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.9.1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
        "from preprocess import gen_data_set, gen_model_input\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "from deepmatch.models import *\n",
        "from deepmatch.utils import sampledsoftmaxloss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQq6O9XAMzPF"
      },
      "source": [
        "# 读取数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcO29zFb21Od",
        "outputId": "ea095585-f5ec-4d1c-9ffa-117531a5ed3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "data_path = \"./\"\n",
        "\n",
        "unames = ['user_id','gender','age','occupation','zip']\n",
        "user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames)\n",
        "rnames = ['user_id','movie_id','rating','timestamp']\n",
        "ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames)\n",
        "mnames = ['movie_id','title','genres']\n",
        "movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames,encoding=\"unicode_escape\")\n",
        "\n",
        "data = pd.merge(pd.merge(ratings,movies),user)#.iloc[:10000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0yCWxQxM3se"
      },
      "source": [
        "# 构建特征列，训练模型，导出embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMOvk_de2ML3",
        "outputId": "24448edc-9100-4a01-c13f-c48d0a6632e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6040/6040 [00:17<00:00, 336.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:435: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 988129 samples\n",
            "Epoch 1/20\n",
            "988129/988129 [==============================] - 24s 25us/sample - loss: 4.4995\n",
            "Epoch 2/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 4.2307\n",
            "Epoch 3/20\n",
            "988129/988129 [==============================] - 25s 25us/sample - loss: 3.8902\n",
            "Epoch 4/20\n",
            "988129/988129 [==============================] - 24s 24us/sample - loss: 3.6825\n",
            "Epoch 5/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.5604\n",
            "Epoch 6/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.4642\n",
            "Epoch 7/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.3803\n",
            "Epoch 8/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.3126\n",
            "Epoch 9/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.2583\n",
            "Epoch 10/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.2177\n",
            "Epoch 11/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.1791\n",
            "Epoch 12/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.1472\n",
            "Epoch 13/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.1246\n",
            "Epoch 14/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.0992\n",
            "Epoch 15/20\n",
            "988129/988129 [==============================] - 24s 24us/sample - loss: 3.0796\n",
            "Epoch 16/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.0601\n",
            "Epoch 17/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.0418\n",
            "Epoch 18/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.0265\n",
            "Epoch 19/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 3.0119\n",
            "Epoch 20/20\n",
            "988129/988129 [==============================] - 23s 23us/sample - loss: 2.9994\n",
            "(6040, 32)\n",
            "(3706, 32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        }
      ],
      "source": [
        "#data = pd.read_csvdata = pd.read_csv(\"./movielens_sample.txt\")\n",
        "sparse_features = [\"movie_id\", \"user_id\",\n",
        "                    \"gender\", \"age\", \"occupation\", \"zip\", ]\n",
        "SEQ_LEN = 50\n",
        "negsample = 0\n",
        "\n",
        "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
        "\n",
        "features = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
        "feature_max_idx = {}\n",
        "for feature in features:\n",
        "    lbe = LabelEncoder()\n",
        "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
        "    feature_max_idx[feature] = data[feature].max() + 1\n",
        "\n",
        "id_count = data['movie_id'].value_counts()  \n",
        "mapdict = {t[0]: i for i, t in\n",
        "            enumerate(sorted([(k, v) for k, v in id_count.to_dict().items()], key=lambda x: x[1], reverse=True))}\n",
        "data['movie_id'] = data['movie_id'].map(mapdict)\n",
        "feature_max_idx['movie_id'] = data['movie_id'].max() + 1\n",
        "\n",
        "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
        "\n",
        "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
        "\n",
        "user_profile.set_index(\"user_id\", inplace=True)\n",
        "\n",
        "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
        "\n",
        "train_set, test_set = gen_data_set(data, negsample)\n",
        "\n",
        "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
        "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
        "\n",
        "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
        "\n",
        "embedding_dim = 32\n",
        "\n",
        "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
        "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
        "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
        "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
        "                        SparseFeat(\"zip\", feature_max_idx['zip'], 16),\n",
        "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
        "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
        "                        ]\n",
        "\n",
        "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n",
        "\n",
        "# 3.Define Model and train\n",
        "\n",
        "K.set_learning_phase(True)\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.__version__ >= '2.0.0':\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=100, user_dnn_hidden_units=(128,64, embedding_dim))\n",
        "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=False,k_max=2,num_sampled=100,user_dnn_hidden_units=(128,64, embedding_dim))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")\n",
        "\n",
        "history = model.fit(train_model_input, train_label,  # train_label,\n",
        "                    batch_size=512, epochs=20, verbose=1, validation_split=0.0, )\n",
        "\n",
        "# 4. Generate user features for testing and full item features for retrieval\n",
        "test_user_model_input = test_model_input\n",
        "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
        "\n",
        "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
        "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
        "\n",
        "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
        "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
        "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
        "\n",
        "print(user_embs.shape)\n",
        "print(item_embs.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_G3KWslKmJo"
      },
      "source": [
        "# 使用faiss进行ANN查找并评估结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SvyQLNVKkcs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2ZNYNBOOqrN",
        "outputId": "442609e8-f94d-42c3-d945-582374a5fa77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TY1l27iJU8U",
        "outputId": "3070ad94-9f84-4b51-d095-18053b84f5ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6040it [00:02, 3004.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "recall 0.3033112582781457\n",
            "hit rate 0.3033112582781457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "test_true_label = {line[0]:[line[2]] for line in test_set}\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from deepmatch.utils import recall_N\n",
        "\n",
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "# faiss.normalize_L2(item_embs)\n",
        "index.add(item_embs)\n",
        "# faiss.normalize_L2(user_embs)\n",
        "D, I = index.search(np.ascontiguousarray(user_embs), 50)\n",
        "s = []\n",
        "hit = 0\n",
        "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
        "    try:\n",
        "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
        "        filter_item = None\n",
        "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
        "        s.append(recall_score)\n",
        "        if test_true_label[uid] in pred:\n",
        "            hit += 1\n",
        "    except:\n",
        "        print(i)\n",
        "print(\"\")\n",
        "print(\"recall\", np.mean(s))\n",
        "print(\"hit rate\", hit / len(test_user_model_input['user_id']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a97TB0obOrRe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "colab_MovieLen1M_YoutubeDNN",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}